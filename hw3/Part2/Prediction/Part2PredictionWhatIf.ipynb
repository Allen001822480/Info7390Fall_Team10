{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import errno\n",
    "import zipfile\n",
    "import csv\n",
    "import urllib\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Constants\n",
    "USERNAME = 'cai.li1@husky.neu.edu'\n",
    "PASSWORD = 'Hf9tN]tm'\n",
    "START = 'Q12001'\n",
    "END = 'Q12011'\n",
    "DIR_NAME = \"data/\"\n",
    "login_page_url = 'https://freddiemac.embs.com/FLoan/secure/auth.php'\n",
    "download_page_url = 'https://freddiemac.embs.com/FLoan/Data/download2.php'\n",
    "REMOVE_UNZIPPED_FILES = False\n",
    "VERBOSE_MODE=True\n",
    "\n",
    "FIELDS = 'ABCDEFGHIJKL0MNOPQRSTUVWXY'\n",
    "\n",
    "## Directory creation if doesn't exist\n",
    "def create_directory(dir_name):\n",
    "    os.getcwd()\n",
    "    if not os.path.exists(dir_name):\n",
    "        try:\n",
    "            os.makedirs(dir_name)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "## gets the zipped files and extrcts the contents into unzipped folder\n",
    "def get_data_from_url(quarter):\n",
    "    print('downloading...')\n",
    "    urllib.request.urlretrieve('https://freddiemac.embs.com/FLoan/Data/historical_data1_' + str(quarter) + '.zip',\n",
    "                               DIR_NAME + str(quarter) + '.zip')\n",
    "    # unzip_files(year)\n",
    "    try:\n",
    "        zip_ref = zipfile.ZipFile(DIR_NAME + str(quarter) + '.zip', 'r')\n",
    "        zip_ref.extractall(DIR_NAME)\n",
    "        zip_ref.close()\n",
    "        \n",
    "        write_into_csv(quarter)\n",
    "    except zipfile.BadZipfile:\n",
    "        print (zipfile.BadZipfile)\n",
    "\n",
    "## Creates a consolidated file\n",
    "\n",
    "## Removes files form the directory\n",
    "def clean_directory(quarter):\n",
    "    os.remove(DIR_NAME + str(quarter) + '.zip')\n",
    "    os.remove(DIR_NAME + \"historical_data1_\" + str(quarter) + '.txt')\n",
    "    os.remove(DIR_NAME + \"historical_data1_time_\" + str(quarter) + '.txt')\n",
    "\n",
    "## Creates a consolidated file\n",
    "def create_csv():\n",
    "    \n",
    "    fields = 'ABCDEFGHIJKL0MNOPQRSTUVWX'\n",
    "    with open(DIR_NAME + '/joinedRawData.csv', 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows([fields])\n",
    "        file.close()\n",
    "    return DIR_NAME + '/joinedRawData.csv'\n",
    "    \n",
    "# Writes the text file lines into csv\n",
    "def write_into_csv(quarter):\n",
    "    with open(DIR_NAME + \"historical_data1_\" + str(quarter) + '.txt', 'r') as sourceFile:\n",
    "        sourcelines = sourceFile.read()\n",
    "        sourcelines = sourcelines.replace(\",\", \"_\")\n",
    "        sourcelines = sourcelines.replace(\"|\", \",\")\n",
    "        sourcelines = sourcelines.replace(\",\\n\", \"\\n\")\n",
    "        \n",
    "#        with open(DIR_NAME + '/joinedRawData.csv', 'a') as destinationFile:\n",
    "#            destinationFile.write(sourcelines)\n",
    "        \n",
    "        with open(DIR_NAME + str(quarter) +'.csv', 'w') as destinationFile:\n",
    "            writer = csv.writer(destinationFile)\n",
    "            writer.writerows([FIELDS])\n",
    "            destinationFile.write(sourcelines)\n",
    "    print('written to csv.')\n",
    "    clean_directory(quarter)\n",
    "    \n",
    "def data_cleaning():\n",
    "    fileList = fileList = glob.glob('data/*.csv')\n",
    "    for file in fileList:\n",
    "        df = pd.read_csv(file,error_bad_lines=False)\n",
    "        print('categorical cleaning...')\n",
    "        for col in ['C','G','H','M','N','O','P','Q','T','U','V','W','X']:\n",
    "            mode = pd.DataFrame(df.groupby(col).size().rename('cnt')).idxmax()[0]\n",
    "            df[col] = df[col].fillna(mode)\n",
    "        print('numerical cleaning...')\n",
    "        for col in ['E','F','I','J','K','L','R']:\n",
    "            dfmean = df[(df[col] != 999)|(df[col] != None)]\n",
    "            mean = int(dfmean[col].mean(axis=0))\n",
    "            df[col] = df[col].fillna(mean)\n",
    "        \n",
    "        filename = \"%sclean.csv\"%file[:-4]\n",
    "        df.to_csv(filename)\n",
    "        \n",
    "        print('cleaning finished.')\n",
    "        \n",
    "        \n",
    "def preprocessing():\n",
    "    fileList = fileList = glob.glob('data/*clean.csv')\n",
    "    cat_columns = ['C','G','H','M','N','O','P','Q','T','U','V','W','X']\n",
    "    num_columns = ['A','B','D','E','F','I','J','K','L','R','0']\n",
    "    for file in fileList:\n",
    "        df = pd.read_csv(file,error_bad_lines=False)\n",
    "        \n",
    "        cat = pd.get_dummies(df[cat_columns])\n",
    "        df = pd.concat([df[num_columns],cat],axis = 1).fillna(0)\n",
    "        \n",
    "        filename = \"%sprocessed.csv\"%file[:-9]\n",
    "        df.to_csv(filename)\n",
    "        \n",
    "        print('processing finished.')\n",
    "        \n",
    "## Main Program Execution\n",
    "def start_execution():\n",
    "    with requests.Session() as sess:\n",
    "        sess.get(login_page_url);\n",
    "        php_session_cookie = sess.cookies['PHPSESSID']\n",
    "        login_payload = {'username' : USERNAME, 'password' : PASSWORD,'cookie':php_session_cookie}\n",
    "        sess.post(login_page_url, data = login_payload)\n",
    "        download_page_payload = {'accept': 'Yes', 'action': 'acceptTandC', 'acceptSubmit': 'Continue', 'cookie': php_session_cookie}\n",
    "        sess.post(download_page_url, data=download_page_payload)\n",
    "        create_directory(DIR_NAME)\n",
    "        #create_csv()\n",
    "        #download files\n",
    "        \n",
    "        #data_cleaning()\n",
    "        #preprocessing()\n",
    "\n",
    "## Calling all the main functions\n",
    "if __name__ == \"__main__\":\n",
    "    start_execution()\n",
    "    print('finished.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use downloaddata.py with proper settings to download the data.\n",
    "\n",
    "import pandas as pd\n",
    "df1 = pd.read_csv('data/Q12001processed.csv')\n",
    "df2 = pd.read_csv('data/Q12002processed.csv')\n",
    "df3 = pd.read_csv('data/Q12003processed.csv')\n",
    "df4 = pd.read_csv('data/Q12004processed.csv')\n",
    "df5 = pd.read_csv('data/Q42005processed.csv')\n",
    "\n",
    "df6 = pd.read_csv('data/Q12006processed.csv')\n",
    "df7 = pd.read_csv('data/Q12007processed.csv')\n",
    "df8 = pd.read_csv('data/Q12008processed.csv')\n",
    "df9 = pd.read_csv('data/Q12009processed.csv')\n",
    "df10 = pd.read_csv('data/Q12010processed.csv')\n",
    "df11 = pd.read_csv('data/Q12011processed.csv')\n",
    "df12 = pd.read_csv('data/Q12012processed.csv')\n",
    "\n",
    "df72 = pd.read_csv('data/Q22007processed.csv')\n",
    "df73 = pd.read_csv('data/Q32007processed.csv')\n",
    "df74 = pd.read_csv('data/Q42007processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def printPerformance(pred,y_test):\n",
    "    print(pred)\n",
    "    print(\"RMSE: %.2f\"\n",
    "          % sqrt(mean_squared_error(y_test, pred)))\n",
    "    print(\"MAPE: %.2f\"\n",
    "          % mean_absolute_percentage_error(y_test, pred)+'%')\n",
    "    print(\"MAE: %.2f\"\n",
    "          % mean_absolute_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What if there is a financial crisis...\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector = VarianceThreshold(0.3)\n",
    "selector2 = VarianceThreshold(0.2)\n",
    "selector3 = VarianceThreshold(0.19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting...\n",
      "fitted.\n",
      "[6.06318916 6.4171998  6.30951532 ... 6.28083239 6.30371094 5.0078125 ]\n",
      "RMSE: 0.35\n",
      "MAPE: 4.32%\n",
      "MAE: 0.26\n",
      "Two Years Later..\n",
      "[7.0546875 7.3203125 7.359375  ... 6.96875   7.0234375 6.96875  ]\n",
      "RMSE: 2.08\n",
      "MAPE: 42.23%\n",
      "MAE: 2.05\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.concat([df7,df72,df73,df74],axis = 0).fillna(0)\n",
    "df_test = pd.concat([df8,df72,df73,df74],axis = 0).fillna(0)\n",
    "\n",
    "#Pipline random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "randomForest = RandomForestRegressor(max_depth= 18, n_estimators = 16, random_state=2)\n",
    "\n",
    "y_train1 = df_train['0']\n",
    "X_train1 = selector.fit_transform(df_train.drop(['0'], axis = 1))\n",
    "\n",
    "y_test1 = df_test['0']\n",
    "X_test1 = selector.fit_transform(df_test.drop(['0'], axis = 1))\n",
    "\n",
    "print('fitting...')\n",
    "randomForest.fit(X_train1,y_train1)\n",
    "print('fitted.')\n",
    "printPerformance(randomForest.predict(X_test1),y_test1)\n",
    "\n",
    "#Two Years Later..\n",
    "print('Two Years Later..')\n",
    "y_test11 = df9['0']\n",
    "X_test11 = selector.fit_transform(df9)\n",
    "randomForest.fit(X_train1,y_train1)\n",
    "printPerformance(randomForest.predict(X_test11),y_test11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What if there is a economy boom...\n",
    "df_train2 = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10],axis = 0).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What if there is a economy boom...\n",
    "df_train2 = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10],axis = 0).fillna(0)\n",
    "y_train2 = df_train2['0']\n",
    "X_train2 = selector2.fit_transform(df_train2.drop(['0'], axis = 1))\n",
    "\n",
    "y_test2 = df12['0']\n",
    "X_test2 = selector3.fit_transform(df12.drop(['0'], axis = 1))\n",
    "print(X_train2.shape)\n",
    "print('fitting...')\n",
    "randomForest.fit(X_train2,y_train2)\n",
    "print('fitted.')\n",
    "printPerformance(randomForest.predict(X_test2), y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.46529516 7.20356593 7.14046844 ... 7.71617187 7.65429688 7.48874423]\n",
      "RMSE: 3.64\n",
      "MAPE: 98.38%\n",
      "MAE: 3.57\n"
     ]
    }
   ],
   "source": [
    "y_test2 = df12['0']\n",
    "X_test2 = selector3.fit_transform(df12.drop(['0'], axis = 1))\n",
    "\n",
    "printPerformance(randomForest.predict(X_test2), y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(313491, 19)\n",
      "fitting...\n",
      "[6.2892583  6.3452474  6.29616552 ... 5.82299758 5.92477492 5.94840169]\n",
      "RMSE: 2.32\n",
      "MAPE: 62.49%\n",
      "MAE: 2.28\n"
     ]
    }
   ],
   "source": [
    "#What if there is a regime change from election\n",
    "y_train3 = df6['0']\n",
    "X_train3 = selector3.fit_transform(df6.drop(['0'],axis = 1))\n",
    "print(X_train3.shape)\n",
    "y_test3 = y_test2\n",
    "X_test3 = X_test2\n",
    "print('fitting...')\n",
    "randomForest.fit(X_train3,y_train3)\n",
    "printPerformance(randomForest.predict(X_test3),y_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
