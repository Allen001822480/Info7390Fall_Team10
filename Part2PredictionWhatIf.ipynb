{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import errno\n",
    "import zipfile\n",
    "import csv\n",
    "import urllib\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Constants\n",
    "USERNAME = 'cai.li1@husky.neu.edu'\n",
    "PASSWORD = 'Hf9tN]tm'\n",
    "START = 'Q12007'\n",
    "END = 'Q22007'\n",
    "DIR_NAME = \"data/\"\n",
    "login_page_url = 'https://freddiemac.embs.com/FLoan/secure/auth.php'\n",
    "download_page_url = 'https://freddiemac.embs.com/FLoan/Data/download2.php'\n",
    "REMOVE_UNZIPPED_FILES = False\n",
    "VERBOSE_MODE=True\n",
    "\n",
    "FIELDS = 'ABCDEFGHIJKL0MNOPQRSTUVWX'\n",
    "\n",
    "## Directory creation if doesn't exist\n",
    "def create_directory(dir_name):\n",
    "    os.getcwd()\n",
    "    if not os.path.exists(dir_name):\n",
    "        try:\n",
    "            os.makedirs(dir_name)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "## gets the zipped files and extrcts the contents into unzipped folder\n",
    "def get_data_from_url(quarter):\n",
    "    print('downloading...')\n",
    "    urllib.request.urlretrieve('https://freddiemac.embs.com/FLoan/Data/historical_data1_' + str(quarter) + '.zip',\n",
    "                               DIR_NAME + str(quarter) + '.zip')\n",
    "    # unzip_files(year)\n",
    "    try:\n",
    "        zip_ref = zipfile.ZipFile(DIR_NAME + str(quarter) + '.zip', 'r')\n",
    "        zip_ref.extractall(DIR_NAME)\n",
    "        zip_ref.close()\n",
    "        \n",
    "        write_into_csv(quarter)\n",
    "    except zipfile.BadZipfile:\n",
    "        print (zipfile.BadZipfile)\n",
    "\n",
    "## Creates a consolidated file\n",
    "\n",
    "## Removes files form the directory\n",
    "def clean_directory(quarter):\n",
    "    os.remove(DIR_NAME + str(quarter) + '.zip')\n",
    "    os.remove(DIR_NAME + \"historical_data1_\" + str(quarter) + '.txt')\n",
    "    os.remove(DIR_NAME + \"historical_data1_time_\" + str(quarter) + '.txt')\n",
    "\n",
    "## Creates a consolidated file\n",
    "def create_csv():\n",
    "    \n",
    "    fields = 'ABCDEFGHIJKL0MNOPQRSTUVWX'\n",
    "    with open(DIR_NAME + '/joinedRawData.csv', 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows([fields])\n",
    "        file.close()\n",
    "    return DIR_NAME + '/joinedRawData.csv'\n",
    "    \n",
    "# Writes the text file lines into csv\n",
    "def write_into_csv(quarter):\n",
    "    with open(DIR_NAME + \"historical_data1_\" + str(quarter) + '.txt', 'r') as sourceFile:\n",
    "        sourcelines = sourceFile.read()\n",
    "        sourcelines = sourcelines.replace(\",\", \"_\")\n",
    "        sourcelines = sourcelines.replace(\"|\", \",\")\n",
    "        sourcelines = sourcelines.replace(\",\\n\", \"\\n\")\n",
    "        \n",
    "#        with open(DIR_NAME + '/joinedRawData.csv', 'a') as destinationFile:\n",
    "#            destinationFile.write(sourcelines)\n",
    "        \n",
    "        with open(DIR_NAME + str(quarter) +'.csv', 'w') as destinationFile:\n",
    "            writer = csv.writer(destinationFile)\n",
    "            writer.writerows([FIELDS])\n",
    "            destinationFile.write(sourcelines)\n",
    "    print('written to csv.')\n",
    "    clean_directory(quarter)\n",
    "    \n",
    "def data_cleaning():\n",
    "    cat_columns = ['C','G','H','M','N','O','P','Q','T','U','V','W','X']\n",
    "    num_columns = ['A','B','D','E','F','I','J','K','L','R','0']\n",
    "    \n",
    "    fileList = fileList = glob.glob('data/*.csv')\n",
    "    for file in fileList:\n",
    "        df = pd.read_csv(file)\n",
    "        print('categorical cleaning...')\n",
    "        for col in cat_columns:\n",
    "            mode = pd.DataFrame(df.groupby(col).size().rename('cnt')).idxmax()[0]\n",
    "            df[col] = df[col].fillna(mode)\n",
    "        print('numerical cleaning...')\n",
    "        for col in ['E','F','I','J','K','L','R']:\n",
    "            dfmean = df[(df[col] != 999)|(df[col] != None)]\n",
    "            mean = int(dfmean[col].mean(axis=0))\n",
    "            df[col] = df[col].fillna(mean)\n",
    "\n",
    "        cat = pd.get_dummies(df[cat_columns])\n",
    "        df = pd.concat([df[num_columns],cat],axis = 1).fillna(0)\n",
    "        \n",
    "        filename = \"%sclean.csv\"%file[:-4]\n",
    "        df.to_csv(filename)\n",
    "        \n",
    "        print('cleaning finished.')\n",
    "        \n",
    "        \n",
    "## Main Program Execution\n",
    "def start_execution():\n",
    "    with requests.Session() as sess:\n",
    "        sess.get(login_page_url);\n",
    "        php_session_cookie = sess.cookies['PHPSESSID']\n",
    "        login_payload = {'username' : USERNAME, 'password' : PASSWORD,'cookie':php_session_cookie}\n",
    "        sess.post(login_page_url, data = login_payload)\n",
    "        download_page_payload = {'accept': 'Yes', 'action': 'acceptTandC', 'acceptSubmit': 'Continue', 'cookie': php_session_cookie}\n",
    "        sess.post(download_page_url, data=download_page_payload)\n",
    "        create_directory(DIR_NAME)\n",
    "#        create_csv()\n",
    "        \n",
    "        get_data_from_url(START)\n",
    "        get_data_from_url(END)\n",
    "        #get_data_from_url('Q32007')\n",
    "        #get_data_from_url('Q42007')\n",
    "        #get_data_from_url('Q12008')\n",
    "        \n",
    "        data_cleaning()\n",
    "\n",
    "## Calling all the main functions\n",
    "if __name__ == \"__main__\":\n",
    "    start_execution()\n",
    "    print('finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df_train = pd.read_csv('data/Q12005clean.csv')\n",
    "df_test = pd.read_csv('data/Q22005clean.csv')\n",
    "df = pd.concat([df_train,df_test],axis = 0)\n",
    "\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "\n",
    "X_train = df.head(df_train.shape[0])\n",
    "y_train = df_train['0']\n",
    "y_train_encoded = lab_enc.fit_transform(y_train)\n",
    "\n",
    "X_test = df.tail(df_test.shape[0])\n",
    "y_test = df_test['0']\n",
    "y_test_encoded = lab_enc.fit_transform(y_test)\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def printPerformance(pred):\n",
    "    print(pred)\n",
    "    print(\"RMSE: %.2f\"\n",
    "          % sqrt(mean_squared_error(y_test, pred)))\n",
    "    print(\"MAPE: %.2f\"\n",
    "          % mean_absolute_percentage_error(y_test, pred)+'%')\n",
    "    print(\"MAE: %.2f\"\n",
    "          % mean_absolute_error(y_test, pred))\n",
    "    \n",
    "#Linear, random forestk, and neural network models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear = LinearRegression()\n",
    "linear.fit(X_train,y_train)\n",
    "linearPredict = linear.predict(X_test)\n",
    "printPerformance(linearPredict)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "randomForest = RandomForestRegressor(max_depth= 18, n_estimators = 16, random_state=2)\n",
    "randomForest.fit(X_train,y_train)\n",
    "randomForestPredict = randomForest.predict(X_test)\n",
    "printPerformance(randomForestPredict)\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "neuralNetwork = MLPRegressor()\n",
    "neuralNetwork.fit(X_train,y_train)\n",
    "neuralNetworkPredict = neuralNetwork.predict(X_test)\n",
    "printPerformance(neuralNetworkPredict)\n",
    "\n",
    "# Build RF classifier to use in feature selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "clf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "\n",
    "#Forward Selection\n",
    "fs = sfs(clf,\n",
    "        k_features=9,\n",
    "        forward=False, \n",
    "        floating=False,\n",
    "        n_jobs=-1,\n",
    "        verbose=2,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        cv=10)\n",
    "fs.fit(X_train,y_train)\n",
    "print('Best MAE score: %.2f' % fs.best_score_ * (-1))\n",
    "print('Best subset:', fs.best_feature_names_)\n",
    "\n",
    "'''\n",
    "#Backward Selection\n",
    "bs = sfs(clf,\n",
    "        k_features=9,\n",
    "        forward=False, \n",
    "        floating=False,\n",
    "        n_jobs=-1,\n",
    "        verbose=2,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        cv=10)\n",
    "bs.fit(X_train,y_train)\n",
    "\n",
    "#Exhaustive Selection\n",
    "\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as efs\n",
    "es = efs(clf, \n",
    "          min_features=8,\n",
    "          max_features=11,\n",
    "          scoring='neg_mean_absolute_error',\n",
    "          n_jobs=-1,\n",
    "          print_progress=True,\n",
    "          cv=8)\n",
    "es.fit(X_train,y_train_encoded)\n",
    "print('Best MAE score: %.2f' % efs.best_score_ * (-1))\n",
    "print('Best subset:', efs.best_feature_names_)\n",
    "'''\n",
    "\n",
    "#Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(linear, X_test, y_test, cv=3)\n",
    "print(scores)\n",
    "scores = cross_val_score(randomForest, X_test, y_test, cv=3)\n",
    "print(scores)\n",
    "scores = cross_val_score(neuralNetwork, X_test, y_test, cv=3)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selector = SelectKBest(f_regression, k=20).fit(X_train,y_train)\n",
    "#k_best_features = X_train.columns.values[selector.get_support()]\n",
    "#\n",
    "##tpot\n",
    "#from tpot import TPOTRegressor\n",
    "#\n",
    "#pipeline_optimizer = TPOTRegressor(generations=5, population_size=20, cv=5,\n",
    "#                                    random_state=42, verbosity=2)\n",
    "#pipeline_optimizer.fit(X_train, y_train)\n",
    "#print(pipeline_optimizer.score(X_test, y_test))\n",
    "#pipeline_optimizer.export('tpot_exported_pipeline.py')\n",
    "\n",
    "#Pipeline Exported\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NOTE: Make sure that the class is labeled '0' in the data file\n",
    "tpot_data = df\n",
    "features = tpot_data.drop('0', axis=1).values\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['0'].values, random_state=42)\n",
    "\n",
    "# Average CV score on the training set was:-6.254833533625527e-26\n",
    "exported_pipeline = LassoLarsCV(normalize=True)\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)\n",
    "printPerformance(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use downloaddata.py with proper settings to download the data.\n",
    "df0 = pd.read_csv('data/Q12000clean.csv')\n",
    "df1 = pd.read_csv('data/Q12001clean.csv')\n",
    "df2 = pd.read_csv('data/Q12002clean.csv')\n",
    "df3 = pd.read_csv('data/Q12003clean.csv')\n",
    "df4 = pd.read_csv('data/Q12004clean.csv')\n",
    "df5 = pd.read_csv('data/Q42005clean.csv')\n",
    "\n",
    "df6 = pd.read_csv('data/Q12006clean.csv')\n",
    "df7 = pd.read_csv('data/Q12007clean.csv')\n",
    "df8 = pd.read_csv('data/Q12008clean.csv')\n",
    "df9 = pd.read_csv('data/Q12009clean.csv')\n",
    "df10 = pd.read_csv('data/Q12010clean.csv')\n",
    "df11 = pd.read_csv('data/Q12011clean.csv')\n",
    "df12 = pd.read_csv('data/Q12012clean.csv')\n",
    "\n",
    "df72 = pd.read_csv('data/Q22007clean.csv')\n",
    "df73 = pd.read_csv('data/Q32007clean.csv')\n",
    "df74 = pd.read_csv('data/Q42007clean.csv')\n",
    "\n",
    "\n",
    "#top features from select k best features.\n",
    "k_best_features = ['D', 'F', 'I', 'K', 'L', 'U', 'C_Y', 'H_I', 'H_P', 'Q_MH', 'T_P',\n",
    " 'W_BANKOFAMERICA_NA', 'W_COUNTRYWIDE', 'W_GMACMTGECORP', 'W_NATLCITYMTGECO',\n",
    " 'W_WASHINGTONMUTUALBANK', 'X_BANKOFAMERICA_NA', 'X_COUNTRYWIDE',\n",
    " 'X_GMACMORTGAGE_LLC', 'X_NATLCITYMTGECO']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What if there is a financial crisis...\n",
    "\n",
    "df_train = pd.concat([df7,df72,df73,df74],axis = 0).sample(frac = 0.1)\n",
    "df_test = pd.concat([df8,df72,df73,df74],axis = 0).sample(frac = 0.1)\n",
    "\n",
    "#Pipline random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "randomForest = RandomForestRegressor(max_depth= 18, n_estimators = 16, random_state=2)\n",
    "\n",
    "X_train = df_train[k_best_features]\n",
    "y_train = df_train['0']\n",
    "X_test = df_test[k_best_features]\n",
    "y_test = df_test['0']\n",
    "\n",
    "randomForest.fit(X_train,y_train)\n",
    "randomForest.predict(X_test)\n",
    "\n",
    "#Two Years Later..\n",
    "X_test = df9[k_best_features]\n",
    "y_test = df9['0']\n",
    "randomForest.fit(X_train,y_train)\n",
    "printPerformance(randomForest.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What if there is a economy boom...\n",
    "\n",
    "df_train = pd.concat([df0,df2,df4,df6,df8,df10,df12],axis = 0).sample(frac = 0.1)\n",
    "\n",
    "X_train = df_train[k_best_features].head(shape)\n",
    "y_train = df_train['0'].tail(df_test.shape)\n",
    "X_test = df12\n",
    "randomForest.fit(X_train,y_train)\n",
    "printPerformance(randomForest.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What if there is a regime change from election\n",
    "\n",
    "X_train = df6[k_best_features].head(shape)\n",
    "y_train = df6['0'].tail(df_test.shape)\n",
    "X_test = df12[k_best_features]\n",
    "y_test = df12['0']\n",
    "\n",
    "randomForest.fit(X_train,y_train)\n",
    "printPerformance(randomForest.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
